2026-01-05 16:24:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'NodeScrapy.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2026-01-05 16:24:34 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/core/downloader/middleware.py:44: ScrapyDeprecationWarning: RandomUserAgentMiddleware.process_request() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(mw.process_request)

2026-01-05 16:24:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2026-01-05 16:24:34 [scrapy.middleware] INFO: Enabled item pipelines:
['NodeScrapy.pipelines.Pipeline', 'NodeScrapy.pipelines.GeoLocPipeline']
2026-01-05 16:24:34 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/pipelines/__init__.py:41: ScrapyDeprecationWarning: Pipeline.open_spider() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.open_spider)

2026-01-05 16:24:34 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/pipelines/__init__.py:44: ScrapyDeprecationWarning: Pipeline.close_spider() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.close_spider)

2026-01-05 16:24:34 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/pipelines/__init__.py:47: ScrapyDeprecationWarning: Pipeline.process_item() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.process_item)

2026-01-05 16:24:34 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/pipelines/__init__.py:41: ScrapyDeprecationWarning: GeoLocPipeline.open_spider() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.open_spider)

2026-01-05 16:24:34 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/pipelines/__init__.py:44: ScrapyDeprecationWarning: GeoLocPipeline.close_spider() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.close_spider)

2026-01-05 16:24:34 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/pipelines/__init__.py:47: ScrapyDeprecationWarning: GeoLocPipeline.process_item() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.process_item)

2026-01-05 16:24:34 [scrapy.core.engine] INFO: Spider opened
2026-01-05 16:24:34 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/core/spidermw.py:490: ScrapyDeprecationWarning: NodeScrapy.spiders.SimpleSpider.SimpleSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2026-01-05 16:24:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2026-01-05 16:24:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2026-01-05 16:24:34 [simple] INFO: clashmeta start
2026-01-05 16:24:34 [simple] INFO: ndnode start
2026-01-05 16:24:34 [simple] INFO: nodev2ray start
2026-01-05 16:24:34 [simple] INFO: nodefree start
2026-01-05 16:24:34 [simple] INFO: v2rayshare start
2026-01-05 16:24:34 [simple] INFO: wenode start
2026-01-05 16:24:34 [simple] INFO: clashmeta found /free-nodes/2025-12-31-clash-meta-node-share.htm on 2026-03-01
2026-01-05 16:24:34 [simple] INFO: clashmeta found /free-nodes/2025-12-27-clash-meta-node-share.htm on 2026-03-01
2026-01-05 16:24:34 [simple] INFO: clashmeta found /free-nodes/2025-12-26-node-share-links.htm on 2026-03-01
2026-01-05 16:24:34 [simple] INFO: clashmeta found /free-nodes/2025-12-25-free-high-speed-nodes.htm on 2026-03-01
2026-01-05 16:24:34 [simple] INFO: clashmeta found /free-nodes/2025-12-24-clash-meta-windows.htm on 2026-03-01
2026-01-05 16:24:34 [simple] INFO: clashmeta found /free-nodes/2025-12-23-clash-meta-node-github.htm on 2026-03-01
2026-01-05 16:24:34 [simple] INFO: clashmeta found /free-nodes/2025-12-22-node-share.htm on 2026-03-01
2026-01-05 16:24:34 [simple] INFO: clashmeta found /free-nodes/2025-12-21-free-high-speed-nodes.htm on 2026-03-01
2026-01-05 16:24:34 [simple] INFO: clashmeta found /free-nodes/2025-12-20-free-clash-meta-node.htm on 2026-03-01
2026-01-05 16:24:34 [simple] INFO: clashmeta found /free-nodes/2025-12-19-node-share.htm on 2026-03-01
2026-01-05 16:24:34 [simple] INFO: clashmeta needs update, accessing https://clash-meta.github.io/free-nodes/2025-12-31-clash-meta-node-share.htm
2026-01-05 16:24:34 [simple] INFO: clashmeta needs update, accessing https://clash-meta.github.io/free-nodes/2025-12-31-clash-meta-node-share.htm
2026-01-05 16:24:34 [simple] INFO: clashmeta needs update, accessing https://clash-meta.github.io/free-nodes/2025-12-27-clash-meta-node-share.htm
2026-01-05 16:24:35 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5318.html on 2026-01-05
2026-01-05 16:24:35 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5319.html on 2026-01-04
2026-01-05 16:24:35 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5320.html on 2026-01-03
2026-01-05 16:24:35 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5321.html on 2026-01-02
2026-01-05 16:24:35 [simple] INFO: v2rayshare found https://v2rayshare.net/p/5322.html on 2026-01-01
2026-01-05 16:24:35 [simple] INFO: v2rayshare is up to date, exiting
2026-01-05 16:24:35 [simple] INFO: v2rayshare is up to date, exiting
2026-01-05 16:24:35 [simple] INFO: v2rayshare is up to date, exiting
2026-01-05 16:24:35 [simple] INFO: nodev2ray found /free-node/2026-1-5-v2ray-windows.htm on 2026-01-05
2026-01-05 16:24:35 [simple] INFO: nodev2ray found /free-node/2026-1-4-v2ray-node.htm on 2026-01-04
2026-01-05 16:24:35 [simple] INFO: nodev2ray found /free-node/2026-1-3-free-ssr-subscribe.htm on 2026-01-03
2026-01-05 16:24:35 [simple] INFO: nodev2ray found /free-node/2026-1-2-v2ray-windows.htm on 2026-01-02
2026-01-05 16:24:35 [simple] INFO: nodev2ray found /free-node/2026-1-1-v2ray-node.htm on 2026-01-01
2026-01-05 16:24:35 [simple] INFO: nodev2ray is up to date, exiting
2026-01-05 16:24:35 [simple] INFO: nodev2ray is up to date, exiting
2026-01-05 16:24:35 [simple] INFO: nodev2ray is up to date, exiting
2026-01-05 16:24:35 [simple] INFO: ndnode found https://www.naidounode.com/n/3482.html on 2026-01-05
2026-01-05 16:24:35 [simple] INFO: ndnode found https://www.naidounode.com/n/3480.html on 2026-01-04
2026-01-05 16:24:35 [simple] INFO: ndnode found https://www.naidounode.com/n/3478.html on 2026-01-03
2026-01-05 16:24:35 [simple] INFO: ndnode found https://www.naidounode.com/n/3476.html on 2026-01-02
2026-01-05 16:24:35 [simple] INFO: ndnode found https://www.naidounode.com/n/3474.html on 2026-01-01
2026-01-05 16:24:35 [simple] INFO: ndnode is up to date, exiting
2026-01-05 16:24:35 [simple] INFO: ndnode is up to date, exiting
2026-01-05 16:24:35 [simple] INFO: ndnode is up to date, exiting
2026-01-05 16:24:35 [simple] INFO: wenode found https://oneclash.cc/a/3547.html on 2026-01-05
2026-01-05 16:24:35 [simple] INFO: wenode found https://oneclash.cc/a/3548.html on 2026-01-04
2026-01-05 16:24:35 [simple] INFO: wenode found https://oneclash.cc/a/3549.html on 2026-01-03
2026-01-05 16:24:35 [simple] INFO: wenode found https://oneclash.cc/a/3550.html on 2026-01-02
2026-01-05 16:24:35 [simple] INFO: wenode found https://oneclash.cc/a/3551.html on 2026-01-01
2026-01-05 16:24:35 [simple] INFO: wenode needs update, accessing https://oneclash.cc/a/3547.html
2026-01-05 16:24:35 [simple] INFO: wenode needs update, accessing https://oneclash.cc/a/3548.html
2026-01-05 16:24:35 [simple] INFO: wenode is up to date, exiting
2026-01-05 16:24:35 [simple] INFO: nodefree found https://nodefree.me/p/3915.html on 2026-01-05
2026-01-05 16:24:35 [simple] INFO: nodefree found https://nodefree.me/p/3916.html on 2026-01-04
2026-01-05 16:24:35 [simple] INFO: nodefree found https://nodefree.me/p/3917.html on 2026-01-03
2026-01-05 16:24:35 [simple] INFO: nodefree found https://nodefree.me/p/3918.html on 2026-01-02
2026-01-05 16:24:35 [simple] INFO: nodefree found https://nodefree.me/p/3919.html on 2026-01-01
2026-01-05 16:24:35 [simple] INFO: nodefree needs update, accessing https://nodefree.me/p/3915.html
2026-01-05 16:24:35 [simple] INFO: nodefree is up to date, exiting
2026-01-05 16:24:35 [simple] INFO: nodefree is up to date, exiting
2026-01-05 16:24:36 [simple] INFO: nodefree found https://nodefree.githubrowcontent.com/2026/01/20260105.txt
2026-01-05 16:24:36 [simple] INFO: nodefree found https://nodefree.githubrowcontent.com/2026/01/20260105.yaml
2026-01-05 16:24:36 [simple] INFO: Pipeline got item nodefree.yaml
2026-01-05 16:24:36 [simple] INFO: Pipeline processed nodefree.yaml
2026-01-05 16:24:36 [simple] INFO: Pipeline got item nodefree.txt
2026-01-05 16:24:36 [simple] INFO: Pipeline processed nodefree.txt
2026-01-05 16:24:36 [scrapy.core.engine] INFO: Closing spider (finished)
2026-01-05 16:24:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5371,
 'downloader/request_count': 16,
 'downloader/request_method_count/GET': 16,
 'downloader/response_bytes': 115958,
 'downloader/response_count': 16,
 'downloader/response_status_count/200': 13,
 'downloader/response_status_count/301': 3,
 'dupefilter/filtered': 1,
 'elapsed_time_seconds': 1.72678,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2026, 1, 5, 16, 24, 36, 447423, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 457920,
 'httpcompression/response_count': 12,
 'item_scraped_count': 2,
 'items_per_minute': 120.0,
 'log_count/INFO': 68,
 'memusage/max': 184193024,
 'memusage/startup': 184193024,
 'request_depth_max': 2,
 'response_received_count': 13,
 'responses_per_minute': 780.0,
 'scheduler/dequeued': 16,
 'scheduler/dequeued/memory': 16,
 'scheduler/enqueued': 16,
 'scheduler/enqueued/memory': 16,
 'start_time': datetime.datetime(2026, 1, 5, 16, 24, 34, 720643, tzinfo=datetime.timezone.utc)}
2026-01-05 16:24:36 [scrapy.core.engine] INFO: Spider closed (finished)
2026-01-05 16:24:55 [scrapy.addons] INFO: Enabled addons:
[]
2026-01-05 16:24:55 [scrapy.extensions.telnet] INFO: Telnet Password: 3de431ec9cfbdc0d
2026-01-05 16:24:55 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logcount.LogCount',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
2026-01-05 16:24:55 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'NodeScrapy',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'scrapy.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'NodeScrapy.spiders',
 'SPIDER_MODULES': ['NodeScrapy.spiders']}
2026-01-05 16:24:55 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'NodeScrapy.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2026-01-05 16:24:55 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/core/downloader/middleware.py:44: ScrapyDeprecationWarning: RandomUserAgentMiddleware.process_request() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(mw.process_request)

2026-01-05 16:24:55 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2026-01-05 16:24:55 [scrapy.middleware] INFO: Enabled item pipelines:
['NodeScrapy.pipelines.Pipeline', 'NodeScrapy.pipelines.GeoLocPipeline']
2026-01-05 16:24:55 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/pipelines/__init__.py:41: ScrapyDeprecationWarning: Pipeline.open_spider() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.open_spider)

2026-01-05 16:24:55 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/pipelines/__init__.py:44: ScrapyDeprecationWarning: Pipeline.close_spider() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.close_spider)

2026-01-05 16:24:55 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/pipelines/__init__.py:47: ScrapyDeprecationWarning: Pipeline.process_item() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.process_item)

2026-01-05 16:24:55 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/pipelines/__init__.py:41: ScrapyDeprecationWarning: GeoLocPipeline.open_spider() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.open_spider)

2026-01-05 16:24:55 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/pipelines/__init__.py:44: ScrapyDeprecationWarning: GeoLocPipeline.close_spider() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.close_spider)

2026-01-05 16:24:55 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/pipelines/__init__.py:47: ScrapyDeprecationWarning: GeoLocPipeline.process_item() requires a spider argument, this is deprecated and the argument will not be passed in future Scrapy versions. If you need to access the spider instance you can save the crawler instance passed to from_crawler() and use its spider attribute.
  self._check_mw_method_spider_arg(pipe.process_item)

2026-01-05 16:24:55 [scrapy.core.engine] INFO: Spider opened
2026-01-05 16:24:55 [py.warnings] WARNING: /opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/scrapy/core/spidermw.py:490: ScrapyDeprecationWarning: NodeScrapy.spiders.SimpleSpider.SimpleSpider (inherited by NodeScrapy.spiders.DecryptSpider.DecryptSpider) defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html
  warn(

2026-01-05 16:24:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2026-01-05 16:24:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2026-01-05 16:24:55 [decrypt] INFO: yudou66 start
2026-01-05 16:24:55 [decrypt] INFO: blues start
2026-01-05 16:24:55 [decrypt] INFO: blues needs update, accessing https://blues2022.blogspot.com/2025/12/202512294kchatgpt4k8kclashv2ray.html
2026-01-05 16:24:55 [decrypt] INFO: blues needs update, accessing https://blues2022.blogspot.com/2025/12/202512284kchatgpt4k8kclashv2ray.html
2026-01-05 16:24:55 [decrypt] INFO: blues needs update, accessing https://blues2022.blogspot.com/2025/12/202512274kchatgpt4k8kclashv2ray.html
2026-01-05 16:24:56 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.yudou6677.top/?m=1>: HTTP status code is not handled or not allowed
2026-01-05 16:24:59 [decrypt] WARNING: blues 0066 got 密碼錯誤
2026-01-05 16:25:00 [decrypt] WARNING: blues 0000 got 密碼錯誤
2026-01-05 16:25:01 [decrypt] WARNING: blues 0011 got 密碼錯誤
2026-01-05 16:25:04 [decrypt] INFO: blues saved new password 0022
2026-01-05 16:25:05 [decrypt] WARNING: blues 0022 got 密碼錯誤
2026-01-05 16:25:07 [decrypt] WARNING: blues 0000 got 密碼錯誤
2026-01-05 16:25:08 [decrypt] WARNING: blues 0011 got 密碼錯誤
2026-01-05 16:25:09 [decrypt] WARNING: blues 0022 got 密碼錯誤
2026-01-05 16:25:12 [decrypt] INFO: blues saved new password 0033
2026-01-05 16:25:13 [decrypt] WARNING: blues 0033 got 密碼錯誤
2026-01-05 16:25:15 [decrypt] WARNING: blues 0000 got 密碼錯誤
2026-01-05 16:25:16 [decrypt] WARNING: blues 0011 got 密碼錯誤
2026-01-05 16:25:17 [decrypt] WARNING: blues 0022 got 密碼錯誤
2026-01-05 16:25:19 [decrypt] WARNING: blues 0033 got 密碼錯誤
2026-01-05 16:25:20 [decrypt] WARNING: blues 0044 got 密碼錯誤
2026-01-05 16:25:22 [decrypt] WARNING: blues 0055 got 密碼錯誤
2026-01-05 16:25:23 [decrypt] WARNING: blues 0066 got 密碼錯誤
2026-01-05 16:25:24 [decrypt] WARNING: blues 0077 got 密碼錯誤
2026-01-05 16:25:25 [decrypt] WARNING: blues 0088 got 密碼錯誤
2026-01-05 16:25:26 [decrypt] WARNING: blues 0099 got 密碼錯誤
2026-01-05 16:25:27 [decrypt] WARNING: blues 1100 got 密碼錯誤
2026-01-05 16:25:29 [decrypt] WARNING: blues 1111 got 密碼錯誤
2026-01-05 16:25:30 [decrypt] WARNING: blues 1122 got 密碼錯誤
2026-01-05 16:25:31 [decrypt] WARNING: blues 1133 got 密碼錯誤
2026-01-05 16:25:32 [decrypt] WARNING: blues 1144 got 密碼錯誤
2026-01-05 16:25:33 [decrypt] WARNING: blues 1155 got 密碼錯誤
2026-01-05 16:25:34 [decrypt] WARNING: blues 1166 got 密碼錯誤
2026-01-05 16:25:36 [decrypt] WARNING: blues 1177 got 密碼錯誤
2026-01-05 16:25:37 [decrypt] WARNING: blues 1188 got 密碼錯誤
2026-01-05 16:25:38 [decrypt] WARNING: blues 1199 got 密碼錯誤
2026-01-05 16:25:40 [decrypt] WARNING: blues 2200 got 密碼錯誤
2026-01-05 16:25:41 [decrypt] WARNING: blues 2211 got 密碼錯誤
2026-01-05 16:25:42 [decrypt] WARNING: blues 2222 got 密碼錯誤
2026-01-05 16:25:43 [decrypt] WARNING: blues 2233 got 密碼錯誤
2026-01-05 16:25:44 [decrypt] WARNING: blues 2244 got 密碼錯誤
2026-01-05 16:25:45 [decrypt] WARNING: blues 2255 got 密碼錯誤
2026-01-05 16:25:46 [decrypt] WARNING: blues 2266 got 密碼錯誤
2026-01-05 16:25:47 [decrypt] WARNING: blues 2277 got 密碼錯誤
2026-01-05 16:25:48 [decrypt] WARNING: blues 2288 got 密碼錯誤
2026-01-05 16:25:49 [decrypt] WARNING: blues 2299 got 密碼錯誤
2026-01-05 16:25:51 [decrypt] WARNING: blues 3300 got 密碼錯誤
2026-01-05 16:25:52 [decrypt] WARNING: blues 3311 got 密碼錯誤
2026-01-05 16:25:53 [decrypt] WARNING: blues 3322 got 密碼錯誤
2026-01-05 16:25:54 [decrypt] WARNING: blues 3333 got 密碼錯誤
2026-01-05 16:25:55 [decrypt] WARNING: blues 3344 got 密碼錯誤
2026-01-05 16:25:56 [decrypt] WARNING: blues 3355 got 密碼錯誤
2026-01-05 16:25:57 [decrypt] WARNING: blues 3366 got 密碼錯誤
2026-01-05 16:25:58 [decrypt] WARNING: blues 3377 got 密碼錯誤
2026-01-05 16:25:59 [decrypt] WARNING: blues 3388 got 密碼錯誤
2026-01-05 16:26:00 [decrypt] WARNING: blues 3399 got 密碼錯誤
2026-01-05 16:26:02 [decrypt] WARNING: blues 4400 got 密碼錯誤
2026-01-05 16:26:03 [decrypt] WARNING: blues 4411 got 密碼錯誤
2026-01-05 16:26:04 [decrypt] WARNING: blues 4422 got 密碼錯誤
2026-01-05 16:26:05 [decrypt] WARNING: blues 4433 got 密碼錯誤
2026-01-05 16:26:06 [decrypt] WARNING: blues 4444 got 密碼錯誤
2026-01-05 16:26:07 [decrypt] WARNING: blues 4455 got 密碼錯誤
2026-01-05 16:26:08 [decrypt] WARNING: blues 4466 got 密碼錯誤
2026-01-05 16:26:09 [decrypt] WARNING: blues 4477 got 密碼錯誤
2026-01-05 16:26:10 [decrypt] WARNING: blues 4488 got 密碼錯誤
2026-01-05 16:26:11 [decrypt] WARNING: blues 4499 got 密碼錯誤
2026-01-05 16:26:13 [decrypt] WARNING: blues 5500 got 密碼錯誤
2026-01-05 16:26:14 [decrypt] WARNING: blues 5511 got 密碼錯誤
2026-01-05 16:26:15 [decrypt] WARNING: blues 5522 got 密碼錯誤
2026-01-05 16:26:16 [decrypt] WARNING: blues 5533 got 密碼錯誤
2026-01-05 16:26:17 [decrypt] WARNING: blues 5544 got 密碼錯誤
2026-01-05 16:26:18 [decrypt] WARNING: blues 5555 got 密碼錯誤
2026-01-05 16:26:20 [decrypt] WARNING: blues 5566 got 密碼錯誤
2026-01-05 16:26:21 [decrypt] WARNING: blues 5577 got 密碼錯誤
2026-01-05 16:26:24 [decrypt] INFO: blues saved new password 5588
2026-01-05 16:26:24 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2026-01-05 16:26:24 [scrapy.core.engine] INFO: Closing spider (finished)
2026-01-05 16:26:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3423,
 'downloader/request_count': 9,
 'downloader/request_method_count/GET': 9,
 'downloader/response_bytes': 205565,
 'downloader/response_count': 9,
 'downloader/response_status_count/200': 4,
 'downloader/response_status_count/302': 4,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 89.176967,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2026, 1, 5, 16, 26, 24, 596629, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1083608,
 'httpcompression/response_count': 5,
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/404': 1,
 'items_per_minute': 0.0,
 'log_count/INFO': 13,
 'log_count/WARNING': 66,
 'memusage/max': 191197184,
 'memusage/startup': 183148544,
 'request_depth_max': 1,
 'response_received_count': 5,
 'responses_per_minute': 3.3707865168539324,
 'scheduler/dequeued': 9,
 'scheduler/dequeued/memory': 9,
 'scheduler/enqueued': 9,
 'scheduler/enqueued/memory': 9,
 'start_time': datetime.datetime(2026, 1, 5, 16, 24, 55, 419662, tzinfo=datetime.timezone.utc)}
2026-01-05 16:26:24 [scrapy.core.engine] INFO: Spider closed (finished)
